# 记忆功能

## 实现思路

实现 LLM 记忆功能的思路是通过额外模块保存和管理对话历史。在每次与模型对话时，将人类和 AI 的历史对话插入到预设的 `chat_history` 占位符中，并将其作为上下文提供给模型。这样就能让模型在每次交互中“记住”之前的对话，确保更连贯的互动。



## 常见记忆模式

### 缓冲记忆

最基础的记忆模式，将所有 Human/Ai 生成的消息全部存储起来，每次需要使用时将保存的所有聊天消息列表传递到 Prompt 中，通过往用户的输入中添加历史对话信息/记忆，可以让 LLM 能理解之前的对话内容，而且这种记忆方式在上下文窗口限制内是无损的。

优点：

1. 无损记忆，用户输入什么内容都会被记忆；
2. 实现方式简单，兼容性最好，所有大模型都支持。

缺点：

1. 直接将存储的所有内容给 LLM，因为大量信息意味着新输入中包含更多的 Token，导致响应时间变慢和成本增加。
2. 当达到 LLM 的令牌数限制时，太长的对话无法被记住。
3. 记忆内容不是无限的，对于上下文长度较小的模型来说，记忆内容会变得极短。



### 缓冲窗口记忆

缓冲窗口记忆只保存最近的几次 Human/Ai 生成的消息，它基于 缓冲记忆 思想，并添加了一个窗口值 k，这意味着只保留一定数量的过去互动，然后“忘记”之前的互动。

优点：

1. 缓冲窗口记忆在限制使用的 Token 数量表现优异。
2. 对小模型也比较友好，不提问比较远的关联内容，一般效果最佳。
3. 实现方式简单，性能优异，所有大模型都支持。

缺点：

1. 缓冲窗口记忆不适合遥远的互动，会忘记之前的“互动”。
2. 部分对话内容长度较大，容易超过 LLM 的上下文限制。



### 令牌缓冲记忆

缓冲窗口记忆只保存限定次数 Human/Ai 生成的消息，它基于 缓冲记忆 思想，并添加了一个令牌数 max_tokens，当聊天历史超过令牌数时，会遗忘之前的互动。

优点：

1. 可以基于大语言模型的上下文长度限制分配记忆长度。
2. 对小模型也比较友好，不提问比较远的关联内容，一般效果最佳。
3. 实现方式简单，性能优异，所有大模型都支持。

缺点：

令牌缓冲记忆不适合遥远的互动，会忘记之前的“互动”。



### 摘要总结记忆

除了将消息传递给 LLM，还可以将消息进行总结，每次只传递总结的信息，而不是完整的消息。这种模式记忆对于较长的对话最有用，可以避免过度使用 Token，因为将过去的信息历史以原文的形式保留在提示中会占用太多的 Token。

优点：

1. 无论是长期还是短期的互动都可以记忆（模糊记忆）。
2. 减少长对话中使用 Token 的数量，能记忆更多轮的对话信息。
3. 长对话时效果明显，虽然最初使用 Token 数量较多，随着对话进行，摘要方法增长速度减慢，与常规缓冲内存模型相比具有优势。

缺点：

1. 虽然能同时记住近期和长远的互动内容，但是记忆的细节部分会丢失；
2. 对于较短的对话可能会增加 Token 使用量。
3. 对话历史的记忆完全依赖于中间摘要 LLM 的能力，需要为摘要 LLM 分配 Token，增加成本且未限制对话长度。



### 摘要缓冲混合记忆

摘要缓冲混合记忆结合了 摘要总结记忆 与 缓冲窗口记忆，它旨在对对话进行摘要总结，同时保留最近互动中的原始内容，但不是简单地清除旧的交互，而是将它们编译成摘要并同时使用，并且使用标记长度而不是交互数量来确定何时清除交互。

优点：

1. 无论是长期还是短期的互动都可以记忆，长期为模糊记忆，短期为精准记忆。
2. 减少长对话中使用 Token 的数量，能记忆更多轮的对话信息。

缺点：

1. 长期互动的内容仍然为模糊记忆。
2. 总结摘要部分完全依赖于中间摘要 LLM 的能力，需要为摘要 LLM 分配 Token，增加成本且未限制对话长度。



### 向量存储库记忆

将记忆存储在向量存储中，并在每次调用时查询前 K 个最匹配的文档。这类记忆模式能记住所有内容，在细节部分比摘要总结要强，但是比缓冲记忆弱，消耗 Token 方面相对平衡。

优点：

1. 拥有比摘要总结更强的细节，比缓冲记忆能记忆更多的内容，甚至无限长度的内容；
2. 消耗的 Token 也相对平衡；

缺点：

1. 性能相比其他模式相对较差，需要额外的 Embedding + 向量数据库支持。
2. 记忆效果受检索功能的影响，好的非常好，差的非常差。



## 组件

### ChatMessageHistory组件

BaseChatMessageHistory是管理历史信息的基类，用于管理历史消息，涵盖了对消息的多种管理：添加消息、清空历史消息、查看历史消息列表、查看历史消息文本等。



### Memory 组件

Memory 组件的基类是 BaseMemory，封装了大量的基础方法。

基于 BaseMemory 基类，衍生出了两个子类 SimpleMemory 和 BaseChatMemory，当 LLM 应用不需要记忆功能，又不想更换代码结构时，可以将记忆组件使用 SimpleMemory 组件进行代替，SimpleMemory 实现了记忆组件的相关方法，但是不存储任何记忆，可以在不修改代码结构的情况下替换记忆组件，实现无记忆功能。



### 缓冲记忆组件的类型

① ConversationBufferMemory：缓冲记忆，最简单，最数据结构和提取算法不做任何处理，将所有对话信息全部存储作为记忆。

② ConversationBufferWindowMemory：缓冲窗口记忆，通过设定 k 值，只保留一定数量（2*k）的对话信息作为历史。

③ ConversationTokenBufferMemory：令牌缓冲记忆，通过设置最大标记数量(max_token_limits)来决定何时清除交互信息，当对话信息超过max_token_limits时，抛弃旧对话信息。

④ ConversationStringBufferMemory:字符串缓冲记忆(早期 LangChain 封装的记忆组件)，等同于缓冲记忆，固定返回字符串。



### 摘要记忆组件的类型

① ConversationSummaryMemory，摘要总结记忆组件，**将传递的历史对话记录总结成摘要进行保存（底层使用 LLM 大语言模型进行总结）**，使用时填充的记忆为 摘要，并非对话数据。这种策略融合了记忆质量和容量的考量，只保留最核心的语义信息，有效减少了冗余，同时质量更高

② ConversationSummaryBufferMemory，摘要缓冲混合记忆，**在不超过 max_token_limit 的限制下，保存对话历史数据，对于超过的部分，进行信息的提取与总结**（底层使用 LLM 大语言模型进行总结），兼顾了精确的短期记忆与模糊的长期记忆。



### 实体记忆组件

实体记忆指的是跟踪对话中提到的实体，并且在对话中记住关于特定实体的既定事实，它提取关于实体的信息（使用LLM），并随着时间的推移建立对该实体的知识（使用LLM），一般使用实体记忆来存储和查询对话中引用的各种信息，比如**人物、地点、事件**等。

在 LangChain 内部封装了一个实体记忆类 ConversationEntityMemory，这个类可以从对话历史中提取实体并生成描述（简单来讲，就是提取关键词+对应的描述），不过 ConversationEntityMemory 预设的 Prompt 过于笨重，而且极度消耗 Token，并且对大模型的要求极高，所以实用度并不高。



### Chain

在许多编程语言和库中，Chain 通常用来描述一系列的操作或函数，**这些操作或函数按照特定的顺序依次执行，前一个操作的输出会作为后一个操作的输入**。这种模式也被称为管道（Pipeline）或链式调用（Chain Calling）。

而在 LangChain 库中，也有 Chain 的概念，用于在复杂场景下，将 LLM 组件、提示词模板、向量存储、记忆、输出解析器等多个组件串联起来一起使用，**在 LCEL 表达式出现之前，LangChain 就为 Chain 设计了相应的接口**，并且为不同场景设计封装了大量 Chain 组件。

在 LangChain 中，存在两种类型的链：

1. **[推荐]**使用 LCEL 构建的链（顺序可执行链）；
2. **[遗产]**通过 Chain 类子类构建的链，这些链不使用 LCEL，而是独立的类。



### RunnableWithMessageHistory

`RunnableWithMessageHistory` 是一个包装器，它会自动加载和保存会话的聊天记录，从而实现多轮对话的上下文连贯性。

RunnableWithMessageHisotry 内部通过传递的运行时配置 session_id 获取到对应的消息历史实例，然后将消息历史实例组装用户输入字典，拼接到原始的 Runnable 可运行链应用中，并为新的 Runnable可运行链应用添加 callback 回调处理器，用于处理存储 LLM 生成的内容，并存储到消息历史记忆中。