# ç¬”è®°

## åè¯è§£é‡Š

###### LLM

LLMå…¨ç§°Large Language Modelï¼Œå³å¤§è¯­è¨€æ¨¡å‹ï¼Œæ˜¯ä¸€ç§ç”¨å¤§é‡æ•°æ®è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹,

ç»™æ¨¡å‹ä¸€äº›è¾“å…¥ï¼Œå®ƒå¯ä»¥é¢„æµ‹å¹¶è¿”å›ç›¸åº”çš„è¾“å‡ºã€‚

###### LangChain

[GitHub - langchain-ai/langchain: ğŸ¦œğŸ”— Build context-aware reasoning applications](https://github.com/langchain-ai/langchain)

æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºç«¯åˆ°ç«¯åº”ç”¨ç¨‹åºçš„è¿‡ç¨‹ï¼Œå®ƒä¹Ÿæ˜¯ReAct(reason+act)è®ºæ–‡çš„è½åœ°å®ç°ã€‚

ç›´è¯‘ï¼š

- **"Lang"**ï¼šæ¥æºäºâ€œLanguageâ€ï¼ˆè¯­è¨€ï¼‰çš„ç¼©å†™ï¼Œè¡¨ç¤ºä¸è¯­è¨€ç›¸å…³çš„å†…å®¹ã€‚
- **"Chain"**ï¼šæ„æ€æ˜¯â€œé“¾æ¡â€æˆ–â€œé“¾å¼ç»“æ„â€ï¼Œè¡¨ç¤ºä¸€ç³»åˆ—æœ‰åºè¿æ¥çš„å…ƒç´ ã€‚

###### Token

Tokenå…¶å®å°±æ˜¯æ–‡æœ¬çš„ç‰‡æ®µï¼Œæ˜¯å¤§æ¨¡å‹è®¡ç®—é•¿åº¦çš„å•ä½ï¼Œå¯¹äºæ±‰å­—ï¼Œå¯ä»¥æ˜¯å­—ã€è¯ã€ç”šè‡³æ˜¯åŠä¸ªå­—æˆ–è€…ä¸‰åˆ†ä¹‹ä¸€ä¸ªå­—ã€‚

###### AIÂ Agent

äººå·¥æ™ºèƒ½ä»£ç†

###### AIGC

AIç”Ÿæˆå†…å®¹

###### AGI

Artificial General Intelligence äººå·¥é€šç”¨æ™ºèƒ½

###### prompts

æç¤ºè¯

###### GPT

Generative Pre-trained Transformer ç”Ÿæˆå‹é¢„è®­ç»ƒå˜æ¢æ¨¡å‹

###### å¤šæ¨¡æ€

ä¸€å¥—æ¨¡å‹å¯ä»¥æ”¯æŒå„ç§å½¢æ€ï¼Œä¾‹ï¼šç”Ÿæˆæ–‡å­—ï¼Œç”Ÿæˆå›¾åƒï¼Œç”ŸæˆéŸ³é¢‘ç­‰ç­‰

###### stable diffusion

å¼€æºå›¾åƒç”Ÿæˆæ¨¡å‹(å¯¹æŠ—ç”Ÿæˆç¥ç»ç½‘ç»œ)

###### Few-shotã€One-shotã€Zero-shot

Few-shot:å¾ˆå°‘çš„æ¡ˆä¾‹

One-shot:ä¸€ä¸ªæ¡ˆä¾‹

Zero-shot:æ²¡æœ‰æ¡ˆä¾‹

æ¡ˆä¾‹ï¼šåœ¨é—®é—®é¢˜æ—¶ï¼Œæ˜¯å¦æœ‰æä¾›ç¤ºä¾‹

###### å’’è¯­

prompt

###### åŸå”±

ç”Ÿæˆ

###### ç‚¼ä¸¹

åœ¨åº•è†œåŸºç¡€ä¸Šåœ¨åŠ ä¸€å±‚è‡ªå·±çš„è®­ç»ƒ

###### ç‚¸ç‚‰

è®­ç»ƒæ¨¡å‹æ—¶å¤±è´¥äº†ï¼Œæœªè¾¾åˆ°é¢„æœŸæ•ˆæœ

###### RAG

Retrieval-Augmented Generation 

æ£€ç´¢å¢å¼ºç”Ÿæˆ

###### LoRA

Low-Rank Adaptation of LLM æ’ä»¶å¼å¾®è°ƒï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–ä»»åŠ¡å®šåˆ¶

###### æ•°æ®è’¸é¦

åŸå§‹å¤§æ•°æ®æµ“ç¼©æˆå°å‹æ•°æ®



## åŸºæœ¬æ¦‚å¿µ

è®­ç»ƒæ¨¡å‹è®­ç»ƒçš„æ˜¯ä»€ä¹ˆï¼Ÿ

è®­ç»ƒæŒ‡çš„æ˜¯å°†å¤§é‡æ–‡æœ¬è¾“å…¥ç»™æ¨¡å‹ï¼Œè¿›è€Œå¾—åˆ°æ¨¡å‹å‚æ•°ï¼Œç›®å‰LLMè®­ç»ƒä¸€èˆ¬ç”¨åˆ°äº†å¤§é‡æ–‡æœ¬ï¼Œä¸€èˆ¬åœ¨ 2T token ä»¥ä¸Šã€‚

## Code

### ç¯å¢ƒå‡†å¤‡

```
pyenv(pythonç‰ˆæœ¬æ¨è3.10)

pip install langchain
pip install openai


æ£€æŸ¥
pip show langchain
pip show openai
```

### ç¤ºä¾‹

```python
from openai import OpenAI

client = OpenAI(api_key="sk-test", base_url="https://api.deepseek.com")


messages = [
{"role": "user", "content": "ä»‹ç»ä¸‹ä½ è‡ªå·±"}
]

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=messages,
    stream=False
)

print(response.choices[0].message.content)
```

### å­—ç¬¦ä¸²æ¨¡ç‰ˆ-PromptTemplate

```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("ä½ å¥½{name},ä½ åœ¨{county}")
str = prompt.format(name="å°å¥‡",county="ä¸­å›½")
print(str)
```

### å¯¹è¯æ¨¡ç‰ˆ-ChatPromptTemplate

```python
from langchain.prompts import ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        ("system", "ä½ å¥½ï¼Œä½ çš„åå­—å«{name}."),
        ("human", "ä½ å¥½{name}ï¼Œä½ åœ¨å¹²å˜›ï¼Ÿ"),
        ("ai", "ä½ å¥½ï¼æˆ‘åœ¨çœ‹åŠ¨æ¼«!"),
    ]
)

str = chat_template.format_messages(name="å°å¥‡")
print(str)
```

ä¹Ÿå¯ä»¥è¿™æ ·

```python
from langchain.schema import SystemMessage
from langchain.schema import HumanMessage
from langchain.schema import AIMessage

# ç›´æ¥åˆ›å»ºæ¶ˆæ¯
system = SystemMessage(
  content="system",
  additional_kwargs={"å§“å": "å°å¥‡"}
)

human = HumanMessage(
  content="human"
)
ai = AIMessage(
  content="AI"
)
[system,human,ai]
```

langchainå†…ç½®æ¨¡ç‰ˆåŒ… 

```python
from langchain.prompts import AIMessagePromptTemplate
from langchain.prompts import SystemMessagePromptTemplate
from langchain.prompts import HumanMessagePromptTemplate
from langchain.prompts import ChatMessagePromptTemplate

prompt = "ä½ å¥½,{name}"

chat_message_prompt = ChatMessagePromptTemplate.from_template(role="AI",template=prompt)
str = chat_message_prompt.format(name="å°å¥‡")

print(str)
```

### è‡ªå®šä¹‰æ¨¡æ¿

```python
from langchain.prompts import StringPromptTemplate
import inspect

def hello_world():
    print("Hello, world!")


PROMPT = """\
å‡½æ•°æ‰“å°ã€‚
å‡½æ•°åç§°: {function_name}
æºä»£ç :
{source_code}
"""


def get_source_code(function_name):
    #è·å¾—æºä»£ç 
    return inspect.getsource(function_name)

#è‡ªå®šä¹‰çš„æ¨¡æ¿class
class CustmPrompt(StringPromptTemplate):

    def format(self, **kwargs) -> str:
        # è·å¾—æºä»£ç 
        source_code = get_source_code(kwargs["function_name"])

        # ç”Ÿæˆæç¤ºè¯æ¨¡æ¿
        prompt = PROMPT.format(
            function_name=kwargs["function_name"].__name__, source_code=source_code
        )
        return prompt

custmPrompt = CustmPrompt(input_variables=["function_name"])
pm = custmPrompt.format(function_name=hello_world)

print(pm)
```

### ä½¿ç”¨jinji2ä¸f-stringæ¥å®ç°æç¤ºè¯æ¨¡æ¿æ ¼å¼åŒ–

```python
##f-stringæ˜¯pythonå†…ç½®çš„ä¸€ç§æ¨¡æ¿å¼•æ“
from langchain.prompts import PromptTemplate

fstring_template = """
ä½ å¥½ï¼Œä½ çš„åå­—æ˜¯{name}
"""

prompt = PromptTemplate.from_template(fstring_template)

msg = prompt.format(name="å°å¥‡")


print(msg)
```

```python
##Jinja2æ˜¯ä¸€ä¸ªçµæ´»ã€é«˜æ•ˆçš„Pythonæ¨¡æ¿å¼•æ“ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ç”Ÿæˆå„ç§æ ‡è®°æ ¼å¼çš„æ–‡æ¡£ã€‚
from langchain.prompts import PromptTemplate

jinja2_template = "ä½ å¥½ï¼Œä½ çš„åå­—æ˜¯{{name}}"
prompt = PromptTemplate.from_template(jinja2_template, template_format="jinja2")

msg = prompt.format(name="å°å¥‡")

print(msg)
```

### ç»„åˆå¼æç¤ºè¯æ¨¡æ¿

```python
from langchain.prompts.pipeline import PipelinePromptTemplate
from langchain.prompts.prompt import PromptTemplate

full_template = """{Character}
{behavior}
{prohibit}"""
full_prompt = PromptTemplate.from_template(full_template)

Character_template = """ä½ æ˜¯{person}."""
Character_prompt = PromptTemplate.from_template(Character_template)

behavior_template = """ä½ éµä»ä»¥ä¸‹çš„è¡Œä¸º:
{behavior_list}
"""
behavior_prompt = PromptTemplate.from_template(behavior_template)

prohibit_template = """ä½ ä¸å…è®¸æœ‰ä»¥ä¸‹è¡Œä¸º:
{prohibit_list}
"""
prohibit_prompt = PromptTemplate.from_template(prohibit_template)


input_prompts = [
    ("Character", Character_prompt),
    ("behavior", behavior_prompt),
    ("prohibit", prohibit_prompt)
]
pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)

print(pipeline_prompt.input_variables)
pm = pipeline_prompt.format(
    person="å°å¥‡",
    behavior_list="è§£å†³é—®é¢˜",
    prohibit_list="ä½ ä¸å¯ä»¥è¯´ä¸çŸ¥é“"
)
print(pm)
ate)
```

### åºåˆ—åŒ–ï¼šä½¿ç”¨æ–‡ä»¶æ¥ç®¡ç†æç¤ºè¯æ¨¡æ¿

- ä¾¿äºå…±äº«

- ä¾¿äºç‰ˆæœ¬ç®¡ç†

- ä¾¿äºå­˜å‚¨

- æ”¯æŒå¸¸è§æ ¼å¼(json/yaml/txt)

```json
{
    "_type":"prompt",
    "input_variables":["name"],
    "template":"ä½ çš„åå­—æ˜¯{name}"
}
```

```yaml
_type: prompt
input_variables:
    ["name"]
template:
    ä½ çš„åå­—æ˜¯{name}
```

```python
from langchain.prompts import load_prompt

#åŠ è½½yamlæ ¼å¼çš„promptæ¨¡ç‰ˆ
prompt = load_prompt("simple_prompt.yaml", encoding="utf-8")
print(prompt.format(name="å°å¥‡"))


#åŠ è½½jsonæ ¼å¼çš„promptæ¨¡ç‰ˆ
prompt = load_prompt("simple_prompt.json", encoding="utf-8")
print(prompt.format(name="å°å¥‡"))
```

### ç¤ºä¾‹é€‰æ‹©å™¨

```python
#æ ¹æ®è¾“å…¥çš„æç¤ºè¯é•¿åº¦ç»¼åˆè®¡ç®—æœ€ç»ˆé•¿åº¦ï¼Œæ™ºèƒ½æˆªå–æˆ–è€…æ·»åŠ æç¤ºè¯çš„ç¤ºä¾‹

from langchain.prompts import PromptTemplate
from langchain.prompts import FewShotPromptTemplate
from langchain.prompts.example_selector import LengthBasedExampleSelector

#å‡è®¾å·²ç»æœ‰è¿™ä¹ˆå¤šçš„æç¤ºè¯ç¤ºä¾‹ç»„ï¼š
examples = [
    {"input":"happy","output":"sad"},
    {"input":"tall","output":"short"},
    {"input":"sunny","output":"gloomy"},
    {"input":"windy","output":"calm"},
    {"input":"é«˜å…´","output":"æ‚²ä¼¤"}
]

#æ„é€ æç¤ºè¯æ¨¡æ¿
example_prompt = PromptTemplate(
    input_variables=["input","output"],
    template="åŸè¯ï¼š{input}\nåä¹‰ï¼š{output}"
)

#è°ƒç”¨é•¿åº¦ç¤ºä¾‹é€‰æ‹©å™¨
example_selector = LengthBasedExampleSelector(
    #ä¼ å…¥æç¤ºè¯ç¤ºä¾‹ç»„
    examples=examples,
    #ä¼ å…¥æç¤ºè¯æ¨¡æ¿
    example_prompt=example_prompt,
    #è®¾ç½®æ ¼å¼åŒ–åçš„æç¤ºè¯æœ€å¤§é•¿åº¦
    max_length=25,
    #å†…ç½®çš„get_text_length,å¦‚æœé»˜è®¤åˆ†è¯è®¡ç®—æ–¹å¼ä¸æ»¡è¶³ï¼Œå¯ä»¥è‡ªå·±æ‰©å±•
    #get_text_length:Callable[[str],int] = lambda x:len(re.split("\n| ",x))
)

#ä½¿ç”¨å°æ ·æœ¬æç¤ºè¯æ¨¡ç‰ˆæ¥å®ç°åŠ¨æ€ç¤ºä¾‹çš„è°ƒç”¨
dynamic_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="ç»™å‡ºæ¯ä¸ªè¾“å…¥è¯çš„åä¹‰è¯",
    suffix="åŸè¯ï¼š{adjective}\nåä¹‰ï¼š",
    input_variables=["adjective"]
)

#å°æ ·æœ¬è·å¾—æ‰€æœ‰ç¤ºä¾‹
print(dynamic_prompt.format(adjective="big"))

#å¦‚æœè¾“å…¥é•¿åº¦å¾ˆé•¿ï¼Œåˆ™æœ€ç»ˆè¾“å‡ºä¼šæ ¹æ®é•¿åº¦è¦æ±‚å‡å°‘
long_string = "big and huge adn massive and large and gigantic and tall and much much much much much much bigger then everyone"
print(dynamic_prompt.format(adjective=long_string))
```

### æ ¹æ®è¾“å…¥ç›¸ä¼¼åº¦é€‰æ‹©ç¤ºä¾‹(æœ€å¤§è¾¹é™…ç›¸å…³æ€§)

- MMRæ˜¯ä¸€ç§åœ¨ä¿¡æ¯æ£€ç´¢ä¸­å¸¸ç”¨çš„æ–¹æ³•ï¼Œå®ƒçš„ç›®æ ‡æ˜¯åœ¨ç›¸å…³æ€§å’Œå¤šæ ·æ€§ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡

- MMRä¼šé¦–å…ˆæ‰¾å‡ºä¸è¾“å…¥æœ€ç›¸ä¼¼ï¼ˆå³ä½™å¼¦ç›¸ä¼¼åº¦æœ€å¤§ï¼‰çš„æ ·æœ¬

- ç„¶ååœ¨è¿­ä»£æ·»åŠ æ ·æœ¬çš„è¿‡ç¨‹ä¸­ï¼Œå¯¹äºä¸å·²é€‰æ‹©æ ·æœ¬è¿‡äºæ¥è¿‘ï¼ˆå³ç›¸ä¼¼åº¦è¿‡é«˜ï¼‰çš„æ ·æœ¬è¿›è¡Œæƒ©ç½š

- MMRæ—¢èƒ½ç¡®ä¿é€‰å‡ºçš„æ ·æœ¬ä¸è¾“å…¥é«˜åº¦ç›¸å…³ï¼Œåˆèƒ½ä¿è¯é€‰å‡ºçš„æ ·æœ¬ä¹‹é—´æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§

- å…³æ³¨å¦‚ä½•åœ¨ç›¸å…³æ€§å’Œå¤šæ ·æ€§ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡

```python
#ä½¿ç”¨MMRæ¥æ£€ç´¢ç›¸å…³ç¤ºä¾‹ï¼Œä»¥ä½¿ç¤ºä¾‹å°½é‡ç¬¦åˆè¾“å…¥

from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import FewShotPromptTemplate,PromptTemplate
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#å‡è®¾å·²ç»æœ‰è¿™ä¹ˆå¤šçš„æç¤ºè¯ç¤ºä¾‹ç»„ï¼š
examples = [
    {"input":"happy","output":"sad"},
    {"input":"tall","output":"short"},
    {"input":"sunny","output":"gloomy"},
    {"input":"windy","output":"calm"},
    {"input":"é«˜å…´","output":"æ‚²ä¼¤"}
]

#æ„é€ æç¤ºè¯æ¨¡ç‰ˆ
example_prompt = PromptTemplate(
    input_variables=["input","output"],
    template="åŸè¯ï¼š{input}\nåä¹‰ï¼š{output}"
)

#è°ƒç”¨MMR
example_selector = MaxMarginalRelevanceExampleSelector.from_examples(
    #ä¼ å…¥ç¤ºä¾‹ç»„
    examples,
    #ä½¿ç”¨openaiçš„åµŒå…¥æ¥åšç›¸ä¼¼æ€§æœç´¢
    OpenAIEmbeddings(openai_api_base=api_base,openai_api_key=api_key),
    #è®¾ç½®ä½¿ç”¨çš„å‘é‡æ•°æ®åº“æ˜¯ä»€ä¹ˆ
    FAISS,
    #ç»“æœæ¡æ•°
    k=2,
)

#ä½¿ç”¨å°æ ·æœ¬æ¨¡ç‰ˆ
mmr_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="ç»™å‡ºæ¯ä¸ªè¾“å…¥è¯çš„åä¹‰è¯",
    suffix="åŸè¯ï¼š{adjective}\nåä¹‰ï¼š",
    input_variables=["adjective"]
)

#å½“æˆ‘ä»¬è¾“å…¥ä¸€ä¸ªæè¿°æƒ…ç»ªçš„è¯è¯­çš„æ—¶å€™ï¼Œåº”è¯¥é€‰æ‹©åŒæ ·æ˜¯æè¿°æƒ…ç»ªçš„ä¸€å¯¹ç¤ºä¾‹ç»„æ¥å¡«å……æç¤ºè¯æ¨¡ç‰ˆ
print(mmr_prompt.format(adjective="éš¾è¿‡"))
```

### æ ¹æ®è¾“å…¥ç›¸ä¼¼åº¦é€‰æ‹©ç¤ºä¾‹(æœ€å¤§ä½™å¼¦ç›¸ä¼¼åº¦)

- ä¸€ç§å¸¸è§çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•

- å®ƒé€šè¿‡è®¡ç®—ä¸¤ä¸ªå‘é‡ï¼ˆåœ¨è¿™é‡Œï¼Œå‘é‡å¯ä»¥ä»£è¡¨æ–‡æœ¬ã€å¥å­æˆ–è¯è¯­ï¼‰ä¹‹é—´çš„ä½™å¼¦å€¼æ¥è¡¡é‡å®ƒä»¬çš„ç›¸ä¼¼åº¦

- ä½™å¼¦å€¼è¶Šæ¥è¿‘1ï¼Œè¡¨ç¤ºä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼

- ä¸»è¦å…³æ³¨çš„æ˜¯å¦‚ä½•å‡†ç¡®è¡¡é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦

```python
# ä½¿ç”¨æœ€å¤§ä½™å¼¦ç›¸ä¼¼åº¦æ¥æ£€ç´¢ç›¸å…³ç¤ºä¾‹ï¼Œä»¥ä½¿ç¤ºä¾‹å°½é‡ç¬¦åˆè¾“å…¥
from langchain.prompts.example_selector import SemanticSimilarityExampleSelector
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import FewShotPromptTemplate, PromptTemplate
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")


example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="åŸè¯: {input}\nåä¹‰: {output}",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

example_selector = SemanticSimilarityExampleSelector.from_examples(
    # ä¼ å…¥ç¤ºä¾‹ç»„.
    examples,
    # ä½¿ç”¨openAIåµŒå…¥æ¥åšç›¸ä¼¼æ€§æœç´¢
    OpenAIEmbeddings(openai_api_key=api_key,openai_api_base=api_base),
    # ä½¿ç”¨Chromaå‘é‡æ•°æ®åº“æ¥å®ç°å¯¹ç›¸ä¼¼ç»“æœçš„è¿‡ç¨‹å­˜å‚¨
    Chroma,
    # ç»“æœæ¡æ•°
    k=1,
)

#ä½¿ç”¨å°æ ·æœ¬æç¤ºè¯æ¨¡æ¿
similar_prompt = FewShotPromptTemplate(
    # ä¼ å…¥é€‰æ‹©å™¨å’Œæ¨¡æ¿ä»¥åŠå‰ç¼€åç¼€å’Œè¾“å…¥å˜é‡
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="ç»™å‡ºæ¯ä¸ªè¾“å…¥è¯çš„åä¹‰è¯",
    suffix="åŸè¯: {adjective}\nåä¹‰:",
    input_variables=["adjective"],
)

# è¾“å…¥ä¸€ä¸ªå½¢å®¹æ„Ÿè§‰çš„è¯è¯­ï¼Œåº”è¯¥æŸ¥æ‰¾è¿‘ä¼¼çš„ happy/sad ç¤ºä¾‹
print(similar_prompt.format(adjective="worried"))
```

### ChatOpenAI

```python
from langchain.chat_models import ChatOpenAI
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

chat = ChatOpenAI(
    model="gpt-4",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base
)

print(chat.predict("ä½ å¥½"))
```

```python
#è°ƒç”¨chatmodelsï¼Œä»¥openaiä¸ºä¾‹

from langchain.chat_models import ChatOpenAI
from langchain.schema.messages import HumanMessage,AIMessage
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

chat = ChatOpenAI(
    model="gpt-4",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base
)

messages = [
    AIMessage(role="system",content="ä½ å¥½ï¼Œæˆ‘æ˜¯adminï¼"),
    HumanMessage(role="user",content="ä½ å¥½adminï¼Œæˆ‘æ˜¯å°å¥‡!"),
    AIMessage(role="system",content="è®¤è¯†ä½ å¾ˆé«˜å…´!"),
    HumanMessage(role="user",content="ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆï¼Ÿ")
]

response = chat.invoke(messages)
print(response)
```

### æµå¼è°ƒç”¨

```python
#LLMç±»å¤§æ¨¡å‹çš„æµå¼è¾“å‡ºæ–¹æ³•

from langchain.llms import OpenAI
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#æ„é€ ä¸€ä¸ªllm
llm = OpenAI(
    model = "gpt-3.5-turbo-instruct",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
    max_tokens=512,
)

for chunk in llm.stream("å†™ä¸€é¦–è¯—"):
    print(chunk,end="",flush=False)
```

```python
#chatmodelsçš„æµå¼è°ƒç”¨æ–¹æ³•
#ä½¿ç”¨cladeæ¨¡å‹
from langchain.chat_models import ChatOpenAI
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(
    model = "claude-3-opus-20240229",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
    max_tokens=512,
)
for chunk in llm.stream("å†™ä¸€é¦–è¯—"):
    print(chunk,end="\n",flush=False)
```

### è¿½è¸ªTokençš„ä½¿ç”¨

```python
#LLMçš„toeknè¿½è¸ª
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#æ„é€ ä¸€ä¸ªllm
llm = OpenAI(
    model = "gpt-3.5-turbo-instruct",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
    max_tokens=512,
)

with get_openai_callback() as cb:
    result = llm.invoke("ç»™æˆ‘è®²ä¸€ä¸ªç¬‘è¯")
    print(result)
    print(cb)
```

```python
#chatmodelsçš„tokenè¿½è¸ª
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(
    model = "gpt-4",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
    max_tokens=512,
)

with get_openai_callback() as cb:
    result = llm.invoke("ç»™æˆ‘è®²ä¸€ä¸ªç¬‘è¯")
    print(result)
    print(cb)
```

### è‡ªå®šä¹‰è¾“å‡º

```python
# ä»¥æ•°æ®æ¨¡å‹æ¥è¾“å‡º
from langchain.llms import  OpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain.pydantic_v1 import BaseModel,Field,validator
from typing import  List
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#æ„é€ LLM
model = OpenAI(
    model = "gpt-3.5-turbo-instruct",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
)

#å®šä¹‰ä¸ªæ•°æ®æ¨¡å‹ï¼Œç”¨æ¥æè¿°æœ€ç»ˆçš„å®ä¾‹ç»“æ„
class Joke(BaseModel):
    setup:str = Field(description="è®¾ç½®ç¬‘è¯çš„é—®é¢˜")
    punchline:str = Field(description="å›ç­”ç¬‘è¯çš„ç­”æ¡ˆ")

    #éªŒè¯é—®é¢˜æ˜¯å¦ç¬¦åˆè¦æ±‚
    @validator("setup")
    def question_mark(cls,field):
        if field[-1] != "ï¼Ÿ":
            raise ValueError("ä¸ç¬¦åˆé¢„æœŸçš„é—®é¢˜æ ¼å¼!")
        return field

#å°†Jokeæ•°æ®æ¨¡å‹ä¼ å…¥
parser = PydanticOutputParser(pydantic_object=Joke)


prompt = PromptTemplate(
    template = "å›ç­”ç”¨æˆ·çš„è¾“å…¥.\n{format_instructions}\n{query}\n",
    input_variables = ["query"],
    partial_variables = {"format_instructions":parser.get_format_instructions()}
)

prompt_and_model = prompt | model
out_put = prompt_and_model.invoke({"query":"ç»™æˆ‘è®²ä¸€ä¸ªç¬‘è¯"})
print("out_put:",out_put)
parser.invoke(out_put)
```

```python
#LLMçš„è¾“å‡ºæ ¼å¼åŒ–æˆpython listå½¢å¼ï¼Œç±»ä¼¼['a','b','c']

from langchain.output_parsers import  CommaSeparatedListOutputParser
from langchain.prompts import  PromptTemplate
from langchain.llms import OpenAI
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#æ„é€ LLM
model = OpenAI(
    model = "gpt-3.5-turbo-instruct",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
)

parser = CommaSeparatedListOutputParser()

prompt = PromptTemplate(
    template = "åˆ—å‡º5ä¸ª{subject}.\n{format_instructions}",
    input_variables = ["subject"],
    partial_variables = {"format_instructions":parser.get_format_instructions()}
)

_input = prompt.format(subject="æ¨èå‡ ä¸ªå¿«é€Ÿå‡è‚¥çš„é€”å¾„")
output = model(_input)
print(output)
#æ ¼å¼åŒ–
parser.parse(output)
```

## RAG

Retrieval-Augmented Generation    æ£€ç´¢å¢å¼ºç”Ÿæˆ

LLMçŸ­æ¿ï¼šçŸ¥è¯†å†…å®¹ä¸å¤Ÿæ–°ï¼Œæ²¡åŠæ³•å®æ—¶å­¦ä¹ å†…å®¹ï¼Œæ›´æ–°æ¯”è¾ƒæ…¢ï¼Œé—­æºæ•°æ®æ— æ³•æ„ŸçŸ¥ã€‚

é€šè¿‡RAGæŠ€æœ¯è®©LLMæ›´èªæ˜ï¼Œå¸®åŠ©è®¡ç®—æœºæ›´å¥½çš„ç†è§£å’Œå›ç­”é—®é¢˜ã€‚

<img src=".\picture\RAG.jpg" style="zoom:80%;" />

langchainä¸­çš„RAGå®ç°

![](D:\project\notes\AI\picture\langchain_rag.jpg)

### Loaderæœºåˆ¶

- åŠ è½½markdown

- åŠ è½½cvs

- åŠ è½½æ–‡ä»¶ç›®å½•

- åŠ è½½html

- åŠ è½½JSON

- åŠ è½½PDF

### æ–‡æ¡£è½¬æ¢å™¨

 æ–‡æ¡£åˆ‡å‰²

åŸç†

1. å°†æ–‡æ¡£åˆ†æˆå°çš„ã€æœ‰æ„ä¹‰çš„å—(å¥å­).

2. å°†å°çš„å—ç»„åˆæˆä¸ºä¸€ä¸ªæ›´å¤§çš„å—ï¼Œç›´åˆ°è¾¾åˆ°ä¸€å®šçš„å¤§å°.

3. ä¸€æ—¦è¾¾åˆ°ä¸€å®šçš„å¤§å°ï¼Œæ¥ç€å¼€å§‹åˆ›å»ºä¸ä¸‹ä¸€ä¸ªå—é‡å çš„éƒ¨åˆ†.

æ–‡æ¡£çš„æ€»ç»“ã€ç²¾ç‚¼å’Œç¿»è¯‘

### å¦‚ä½•å¤„ç†é•¿æ–‡æœ¬åˆ‡åˆ†ä¿¡æ¯ä¸¢å¤±é—®é¢˜?

è®ºæ–‡èµ„æ–™ï¼š

[[2307.03172] Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)

å¯¹äºé•¿æ–‡æœ¬ï¼š10ä¸ªä»¥ä¸Šå—æ£€ç´¢æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œç›¸å…³ä¿¡æ¯åœ¨å¤´å°¾æ€§èƒ½æœ€é«˜

è§£å†³æ–¹æ¡ˆï¼šæ£€ç´¢-æ’åº(é—®é¢˜ç›¸å…³æ’åœ¨å¤´å°¾)-ä½¿ç”¨

### æ–‡æœ¬å‘é‡åŒ–

EmbeddingåµŒå…¥å¯ä»¥è®©ä¸€ç»„æ–‡æœ¬æˆ–è€…ä¸€æ®µè¯ä»¥å‘é‡æ¥è¡¨ç¤ºï¼Œä»è€Œå¯ä»¥è®©æˆ‘ä»¬åœ¨å‘é‡ç©ºé—´ä¸­è¿›è¡Œè¯­ä¹‰æœç´¢ä¹‹ç±»çš„æ“ä½œï¼Œä»è€Œå¤§å¹…æå‡å­¦ä¹ èƒ½åŠ›

### å‘é‡æ•°æ®åº“

å‘é‡æ•°æ®åº“ä¸»è¦åœ¨ä¿¡æ¯ç´¢å¼•å’Œä¿¡æ¯æ£€ç´¢ä¸¤ä¸ªæ–¹å‘åº”ç”¨

å‘é‡æ•°æ®:ç”¨ç©ºé—´æ¥æè¿°é«˜ç»´æ•°æ®ï¼Œä»¥è·ç¦»æ¥åˆ¤æ–­äº²ç–

å‘é‡æ•°æ®åº“å¤„ç†é«˜ç»´æ•°æ®æ–¹é¢å…·å¤‡å¤©ç„¶ä¼˜åŠ¿

æ˜¯å›¾å½¢å¤„ç†ã€æ¨èç³»ç»Ÿçš„èƒŒåè‹±é›„

ç®¡ç†:ä»¥åŸå§‹æ•°æ®å½¢å¼å¤„ç†æ•°æ®ï¼Œç®¡ç†æ›´æœ‰æ•ˆ

å­˜å‚¨:èƒ½å¤Ÿå­˜å‚¨å‘é‡æ•°æ®ä»¥åŠAIéœ€è¦çš„é«˜ç»´æ•°æ®

æ£€ç´¢:å¯ä»¥é«˜æ•ˆæ£€ç´¢æ•°æ®ï¼ŒAIéå¸¸éœ€è¦çš„ç‰¹æ€§
è®©AIå…·å¤‡äº†è®°å¿†èƒ½åŠ›

![](D:\project\notes\AI\picture\database.jpg)

### å‡ ç§æ£€ç´¢ä¼˜åŒ–æ–¹å¼

1.ä½¿ç”¨å¤šé‡æŸ¥è¯¢æé«˜æ–‡æ¡£æ£€ç´¢ç²¾ç¡®åº¦

2.ä½¿ç”¨ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢é™ä½å†—ä½™ä¿¡æ¯

3.åœ¨å‘é‡å­˜å‚¨é‡Œä½¿ç”¨æœ€å¤§è¾¹é™…ç›¸ä¼¼æ€§ï¼ˆMMRï¼‰å’Œç›¸ä¼¼æ€§æ‰“åˆ†

## å¤§æ¨¡å‹è®°å¿†å¢å¼º

### å››ç§å†…ç½®é“¾

#### LLMChain

æœ€å¸¸ç”¨çš„é“¾å¼
æç¤ºè¯æ¨¡æ¿+(LLM/chatModes)+è¾“å‡ºæ ¼å¼åŒ–å™¨(å¯é€‰)

æ”¯æŒå¤šç§è°ƒç”¨æ–¹å¼

#### SequentialChain

é¡ºåºæ‰§è¡Œ

å°†å‰ä¸€ä¸ªLLMçš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªLLMçš„è¾“å…¥

#### RouterChain

è·¯ç”±é“¾æ”¯æŒåˆ›å»ºä¸€ä¸ªéç¡®å®šæ€§é“¾ï¼Œç”±LLMæ¥é€‰æ‹©ä¸‹ä¸€æ­¥

é“¾å†…çš„å¤šä¸ªpromptsæ¨¡æ¿æè¿°äº†ä¸åŒçš„æç¤ºè¯·æ±‚

#### Transformation

æ”¯æŒå¯¹ä¼ é€’éƒ¨ä»¶çš„è½¬æ¢

æ¯”å¦‚å°†ä¸€ä¸ªè¶…é•¿æ–‡æœ¬è¿‡æ»¤è½¬æ¢ä¸ºä»…åŒ…å«å‰ä¸‰ä¸ªæ®µè½ï¼Œç„¶åæäº¤ç»™LLM

#### å®˜æ–¹æä¾›çš„é“¾

[langchain-hub/chains at master Â· hwchase17/langchain-hub Â· GitHub](https://github.com/hwchase17/langchain-hub/tree/master/chains)

### å››ç§å¤„ç†æ–‡æ¡£é¢„åˆ¶é“¾

#### stuffChain

æœ€å¸¸è§çš„æ–‡æ¡£é“¾ï¼Œå°†æ–‡æ¡£ç›´æ¥å¡è¿›promptä¸­ï¼Œä¸ºLLMå›ç­”é—®é¢˜æä¾›ä¸Šä¸‹æ–‡èµ„æ–™ï¼Œé€‚åˆå°æ–‡æ¡£åœºæ™¯

#### refine

é€šè¿‡å¾ªç¯å¼•ç”¨LLMï¼Œå°†æ–‡æ¡£ä¸æ–­æŠ•å–‚ï¼Œå¹¶äº§ç”Ÿå„ç§ä¸­é—´ç­”æ¡ˆï¼Œé€‚åˆé€»è¾‘æœ‰ä¸Šä¸‹æ–‡å…³è”çš„æ–‡æ¡£ï¼Œä¸é€‚åˆäº¤å‰å¼•ç”¨çš„æ–‡æ¡£

#### Map reduce

å…ˆå°†æ¯ä¸ªæ–‡æ¡£æˆ–æ–‡æ¡£å—åˆ†åˆ«æŠ•å–‚ç»™LLMï¼Œå¹¶å¾—åˆ°ç»“æœé›†ï¼ˆMapæ­¥éª¤ï¼‰ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ–‡æ¡£åˆå¹¶é“¾ï¼Œè·å¾—ä¸€ä¸ªè¾“å‡ºç»“æœï¼ˆReduceæ­¥éª¤ï¼‰

#### Map re-rank

å…ˆå°†æ¯ä¸ªæ–‡æ¡£æˆ–æ–‡æ¡£å—æŠ•å–‚ç»™LLM,å¹¶å¯¹æ¯ä¸ªæ–‡æ¡£æˆ–æ–‡æ¡£å—ç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆè¿›è¡Œæ‰“åˆ†ï¼Œç„¶åå°†æ‰“åˆ†æœ€é«˜çš„æ–‡æ¡£æˆ–æ–‡æ¡£å—ä½œä¸ºæœ€ç»ˆç­”æ¡ˆè¿”å›

### ä¸åŒçš„Memoryå·¥å…·

åˆ©ç”¨å†…å­˜å®ç°çŸ­æ—¶è®°å¿†

åˆ©ç”¨Entity memoryæ„å»ºå®ä½“è®°å¿†

åˆ©ç”¨çŸ¥è¯†å›¾è°±æ¥æ„å»ºè®°å¿†

åˆ©ç”¨å¯¹è¯æ‘˜è¦æ¥å…¼å®¹å†…å­˜ä¸­çš„é•¿å¯¹è¯

ä½¿ç”¨tokenæ¥åˆ·æ–°å†…å­˜ç¼“å†²åŒº

ä½¿ç”¨å‘é‡æ•°æ®åº“å®ç°é•¿æ—¶è®°å¿†

### åœ¨é“¾ä¸Šä½¿ç”¨è®°å¿†

- LLMChain

- ConversationChain

- è‡ªå®šä¹‰

- åŒä¸€ä¸ªé“¾åˆå¹¶ä½¿ç”¨å¤šä¸ªè®°å¿†

- ç»™ä¸€ä¸ªå¤šå‚æ•°é“¾å¢åŠ è®°å¿†

## Agents

### å‡ ç§ä¸åŒçš„agentç±»å‹

- ZERO_SHOT_REACT_DESCRIPTION

- CHAT_ZERO_SHOT_REACT_DESCRIPTION

- CONVERSATIONAL_REACT_DESCRIPTION

- CHAT_CONVERSATIONAL_REACT_DESCRIPTION

- OPENAI_FUNCTIONS

- STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION

ZERO_SHOT_REACT_DESCRIPTION

é›¶æ ·æœ¬å¢å¼ºå¼ç”Ÿæˆï¼Œå³åœ¨æ²¡æœ‰ç¤ºä¾‹çš„æƒ…å†µä¸‹å¯ä»¥è‡ªä¸»çš„è¿›è¡Œå¯¹è¯çš„ç±»å‹ã€‚https://blog.csdn.net/zcyzcyjava/article/details/127006287 [é›¶æ ·æœ¬ã€å•æ ·æœ¬ã€å°‘æ ·æœ¬]

CHAT_ZERO_SHOT_REACT_DESCRIPTION ä½¿ç”¨äº†chatmodel

CONVERSATIONAL_REACT_DESCRIPTION ä¸€ä¸ªå¯¹è¯å‹çš„agentï¼Œè¿™ä¸ªagentè¦æ±‚ä¸memoryä¸€èµ·ä½¿ç”¨

CHAT_CONVERSATIONAL_REACT_DESCRIPTION ä½¿ç”¨äº†chatmodel

OPENAI_FUNCTIONSï¼Œä½¿ç”¨openaiçš„å‡½æ•°è°ƒç”¨æ¥å®ç°çš„ï¼Œåªæ”¯æŒopenaiæ¨¡å‹ã€‚

STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION å¯¹è¾“å‡ºåšäº†ç»“æ„åŒ–å¤„ç†

### å¦‚ä½•ç»™agentæ­£ç¡®çš„å¢åŠ è®°å¿†

- å°†memoryæ’å…¥åˆ°æç¤ºè¯æ¨¡æ¿ä¸­

### åœ¨agentä¸toolä¹‹é—´å…±äº«è®°å¿†

- è‡ªå®šä¹‰ä¸€ä¸ªå·¥å…·ç”¨æ¥LLMChainæ¥æ€»ç»“å†…å®¹

- ä½¿ç”¨readonlymemoryæ¥å…±äº«è®°å¿†

- è§‚å¯Ÿå…±äº«ä¸ä¸å…±äº«çš„åŒºåˆ«

### å¦‚ä½•åŠ è½½ä½¿ç”¨tool

- åŠ è½½é¢„åˆ¶toolçš„æ–¹æ³•

- å‡ ç§toolçš„ä½¿ç”¨æ–¹å¼

langchainé¢„åˆ¶äº†å¤§é‡çš„toolsï¼ŒåŸºæœ¬è¿™äº›å·¥å…·èƒ½æ»¡è¶³å¤§éƒ¨åˆ†éœ€æ±‚ï¼Œhttps://github.com/langchain-ai/langchain/tree/v0.0.352/docs/docs/integrations/tools

```
SerpAPI
æœ€å¸¸è§çš„èšåˆæœç´¢å¼•æ“ https://serper.dev/dashboard

ä½¿ç”¨Dall-E
Dall-Eæ˜¯openaiå‡ºå“çš„æ–‡åˆ°å›¾AIå¤§æ¨¡å‹

Eleven Labs Text2Speech
ElevenLabs æ˜¯éå¸¸ä¼˜ç§€çš„TTSåˆæˆAPI

GraphQL
ä¸€ç§apiæŸ¥è¯¢è¯­è¨€ï¼Œç±»ä¼¼sqlï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥æŸ¥è¯¢å¥ˆé£çš„æ•°æ®åº“ï¼ŒæŸ¥æ‰¾ä¸€ä¸‹å’Œæ˜Ÿçƒå¤§æˆ˜ç›¸å…³çš„ç”µå½±ï¼Œ
APIåœ°å€https://swapi-graphql.netlify.app/.netlify/functions/index
```

### Tookit

tookitæ˜¯langchainå·²ç»å°è£…å¥½çš„ä¸€ç³»åˆ—å·¥å…·ï¼Œä¸€ä¸ªå·¥å…·åŒ…æ˜¯ä¸€ç»„å·¥å…·æ¥ç»„åˆå®Œæˆç‰¹å®šçš„ä»»åŠ¡

Azureè®¤çŸ¥æœåŠ¡ https://portal.azure.com/#allservices

- AzureCogsFormRecognizerToolï¼šä»æ–‡æ¡£é‡Œæå–æ–‡æœ¬

- AzureCogsSpeech2TextToolï¼šè¯­éŸ³åˆ°æ–‡æœ¬

- AzureCogsText2SpeechToolï¼šæ–‡æœ¬åˆ°è¯­éŸ³

### LCEL(LangChain Expression Language)

ä¸€ç§åœ¨langchainä¹‹ä¸Šå°è£…çš„é«˜çº§è§£é‡Šè¯­è¨€

ç®€åŒ–é“¾æ¡å¼€å‘ï¼Œæ”¯æŒçœŸå®ç”Ÿäº§ç¯å¢ƒè€Œå‘æ˜

## å¸¸ç”¨ç½‘ç«™

æ¨¡å‹ä»“åº“

https://huggingface.co/
