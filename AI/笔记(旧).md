# 笔记

## 名词解释

###### LLM

LLM全称Large Language Model，即大语言模型，是一种用大量数据训练的深度学习模型,

给模型一些输入，它可以预测并返回相应的输出。

###### LangChain

[GitHub - langchain-ai/langchain: 🦜🔗 Build context-aware reasoning applications](https://github.com/langchain-ai/langchain)

是一个开源框架，旨在简化使用大型语言模型构建端到端应用程序的过程，它也是ReAct(reason+act)论文的落地实现。

直译：

- **"Lang"**：来源于“Language”（语言）的缩写，表示与语言相关的内容。
- **"Chain"**：意思是“链条”或“链式结构”，表示一系列有序连接的元素。

###### Token

Token其实就是文本的片段，是大模型计算长度的单位，对于汉字，可以是字、词、甚至是半个字或者三分之一个字。

###### AI Agent

人工智能代理

###### AIGC

AI生成内容

###### AGI

Artificial General Intelligence 人工通用智能

###### prompts

提示词

###### GPT

Generative Pre-trained Transformer 生成型预训练变换模型

###### 多模态

一套模型可以支持各种形态，例：生成文字，生成图像，生成音频等等

###### stable diffusion

开源图像生成模型(对抗生成神经网络)

###### Few-shot、One-shot、Zero-shot

Few-shot:很少的案例

One-shot:一个案例

Zero-shot:没有案例

案例：在问问题时，是否有提供示例

###### 咒语

prompt

###### 吟唱

生成

###### 炼丹

在底膜基础上在加一层自己的训练

###### 炸炉

训练模型时失败了，未达到预期效果

###### RAG

Retrieval-Augmented Generation 

检索增强生成

###### LoRA

Low-Rank Adaptation of LLM 插件式微调，对大语言模型进行个性化任务定制

###### 数据蒸馏

原始大数据浓缩成小型数据



## 基本概念

训练模型训练的是什么？

训练指的是将大量文本输入给模型，进而得到模型参数，目前LLM训练一般用到了大量文本，一般在 2T token 以上。

## Code

### 环境准备

```
pyenv(python版本推荐3.10)

pip install langchain
pip install openai


检查
pip show langchain
pip show openai
```

### 示例

```python
from openai import OpenAI

client = OpenAI(api_key="sk-test", base_url="https://api.deepseek.com")


messages = [
{"role": "user", "content": "介绍下你自己"}
]

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=messages,
    stream=False
)

print(response.choices[0].message.content)
```

### 字符串模版-PromptTemplate

```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("你好{name},你在{county}")
str = prompt.format(name="小奇",county="中国")
print(str)
```

### 对话模版-ChatPromptTemplate

```python
from langchain.prompts import ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        ("system", "你好，你的名字叫{name}."),
        ("human", "你好{name}，你在干嘛？"),
        ("ai", "你好！我在看动漫!"),
    ]
)

str = chat_template.format_messages(name="小奇")
print(str)
```

也可以这样

```python
from langchain.schema import SystemMessage
from langchain.schema import HumanMessage
from langchain.schema import AIMessage

# 直接创建消息
system = SystemMessage(
  content="system",
  additional_kwargs={"姓名": "小奇"}
)

human = HumanMessage(
  content="human"
)
ai = AIMessage(
  content="AI"
)
[system,human,ai]
```

langchain内置模版包 

```python
from langchain.prompts import AIMessagePromptTemplate
from langchain.prompts import SystemMessagePromptTemplate
from langchain.prompts import HumanMessagePromptTemplate
from langchain.prompts import ChatMessagePromptTemplate

prompt = "你好,{name}"

chat_message_prompt = ChatMessagePromptTemplate.from_template(role="AI",template=prompt)
str = chat_message_prompt.format(name="小奇")

print(str)
```

### 自定义模板

```python
from langchain.prompts import StringPromptTemplate
import inspect

def hello_world():
    print("Hello, world!")


PROMPT = """\
函数打印。
函数名称: {function_name}
源代码:
{source_code}
"""


def get_source_code(function_name):
    #获得源代码
    return inspect.getsource(function_name)

#自定义的模板class
class CustmPrompt(StringPromptTemplate):

    def format(self, **kwargs) -> str:
        # 获得源代码
        source_code = get_source_code(kwargs["function_name"])

        # 生成提示词模板
        prompt = PROMPT.format(
            function_name=kwargs["function_name"].__name__, source_code=source_code
        )
        return prompt

custmPrompt = CustmPrompt(input_variables=["function_name"])
pm = custmPrompt.format(function_name=hello_world)

print(pm)
```

### 使用jinji2与f-string来实现提示词模板格式化

```python
##f-string是python内置的一种模板引擎
from langchain.prompts import PromptTemplate

fstring_template = """
你好，你的名字是{name}
"""

prompt = PromptTemplate.from_template(fstring_template)

msg = prompt.format(name="小奇")


print(msg)
```

```python
##Jinja2是一个灵活、高效的Python模板引擎，可以方便地生成各种标记格式的文档。
from langchain.prompts import PromptTemplate

jinja2_template = "你好，你的名字是{{name}}"
prompt = PromptTemplate.from_template(jinja2_template, template_format="jinja2")

msg = prompt.format(name="小奇")

print(msg)
```

### 组合式提示词模板

```python
from langchain.prompts.pipeline import PipelinePromptTemplate
from langchain.prompts.prompt import PromptTemplate

full_template = """{Character}
{behavior}
{prohibit}"""
full_prompt = PromptTemplate.from_template(full_template)

Character_template = """你是{person}."""
Character_prompt = PromptTemplate.from_template(Character_template)

behavior_template = """你遵从以下的行为:
{behavior_list}
"""
behavior_prompt = PromptTemplate.from_template(behavior_template)

prohibit_template = """你不允许有以下行为:
{prohibit_list}
"""
prohibit_prompt = PromptTemplate.from_template(prohibit_template)


input_prompts = [
    ("Character", Character_prompt),
    ("behavior", behavior_prompt),
    ("prohibit", prohibit_prompt)
]
pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)

print(pipeline_prompt.input_variables)
pm = pipeline_prompt.format(
    person="小奇",
    behavior_list="解决问题",
    prohibit_list="你不可以说不知道"
)
print(pm)
ate)
```

### 序列化：使用文件来管理提示词模板

- 便于共享

- 便于版本管理

- 便于存储

- 支持常见格式(json/yaml/txt)

```json
{
    "_type":"prompt",
    "input_variables":["name"],
    "template":"你的名字是{name}"
}
```

```yaml
_type: prompt
input_variables:
    ["name"]
template:
    你的名字是{name}
```

```python
from langchain.prompts import load_prompt

#加载yaml格式的prompt模版
prompt = load_prompt("simple_prompt.yaml", encoding="utf-8")
print(prompt.format(name="小奇"))


#加载json格式的prompt模版
prompt = load_prompt("simple_prompt.json", encoding="utf-8")
print(prompt.format(name="小奇"))
```

### 示例选择器

```python
#根据输入的提示词长度综合计算最终长度，智能截取或者添加提示词的示例

from langchain.prompts import PromptTemplate
from langchain.prompts import FewShotPromptTemplate
from langchain.prompts.example_selector import LengthBasedExampleSelector

#假设已经有这么多的提示词示例组：
examples = [
    {"input":"happy","output":"sad"},
    {"input":"tall","output":"short"},
    {"input":"sunny","output":"gloomy"},
    {"input":"windy","output":"calm"},
    {"input":"高兴","output":"悲伤"}
]

#构造提示词模板
example_prompt = PromptTemplate(
    input_variables=["input","output"],
    template="原词：{input}\n反义：{output}"
)

#调用长度示例选择器
example_selector = LengthBasedExampleSelector(
    #传入提示词示例组
    examples=examples,
    #传入提示词模板
    example_prompt=example_prompt,
    #设置格式化后的提示词最大长度
    max_length=25,
    #内置的get_text_length,如果默认分词计算方式不满足，可以自己扩展
    #get_text_length:Callable[[str],int] = lambda x:len(re.split("\n| ",x))
)

#使用小样本提示词模版来实现动态示例的调用
dynamic_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="给出每个输入词的反义词",
    suffix="原词：{adjective}\n反义：",
    input_variables=["adjective"]
)

#小样本获得所有示例
print(dynamic_prompt.format(adjective="big"))

#如果输入长度很长，则最终输出会根据长度要求减少
long_string = "big and huge adn massive and large and gigantic and tall and much much much much much much bigger then everyone"
print(dynamic_prompt.format(adjective=long_string))
```

### 根据输入相似度选择示例(最大边际相关性)

- MMR是一种在信息检索中常用的方法，它的目标是在相关性和多样性之间找到一个平衡

- MMR会首先找出与输入最相似（即余弦相似度最大）的样本

- 然后在迭代添加样本的过程中，对于与已选择样本过于接近（即相似度过高）的样本进行惩罚

- MMR既能确保选出的样本与输入高度相关，又能保证选出的样本之间有足够的多样性

- 关注如何在相关性和多样性之间找到一个平衡

```python
#使用MMR来检索相关示例，以使示例尽量符合输入

from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import FewShotPromptTemplate,PromptTemplate
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#假设已经有这么多的提示词示例组：
examples = [
    {"input":"happy","output":"sad"},
    {"input":"tall","output":"short"},
    {"input":"sunny","output":"gloomy"},
    {"input":"windy","output":"calm"},
    {"input":"高兴","output":"悲伤"}
]

#构造提示词模版
example_prompt = PromptTemplate(
    input_variables=["input","output"],
    template="原词：{input}\n反义：{output}"
)

#调用MMR
example_selector = MaxMarginalRelevanceExampleSelector.from_examples(
    #传入示例组
    examples,
    #使用openai的嵌入来做相似性搜索
    OpenAIEmbeddings(openai_api_base=api_base,openai_api_key=api_key),
    #设置使用的向量数据库是什么
    FAISS,
    #结果条数
    k=2,
)

#使用小样本模版
mmr_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="给出每个输入词的反义词",
    suffix="原词：{adjective}\n反义：",
    input_variables=["adjective"]
)

#当我们输入一个描述情绪的词语的时候，应该选择同样是描述情绪的一对示例组来填充提示词模版
print(mmr_prompt.format(adjective="难过"))
```

### 根据输入相似度选择示例(最大余弦相似度)

- 一种常见的相似度计算方法

- 它通过计算两个向量（在这里，向量可以代表文本、句子或词语）之间的余弦值来衡量它们的相似度

- 余弦值越接近1，表示两个向量越相似

- 主要关注的是如何准确衡量两个向量的相似度

```python
# 使用最大余弦相似度来检索相关示例，以使示例尽量符合输入
from langchain.prompts.example_selector import SemanticSimilarityExampleSelector
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import FewShotPromptTemplate, PromptTemplate
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")


example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="原词: {input}\n反义: {output}",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

example_selector = SemanticSimilarityExampleSelector.from_examples(
    # 传入示例组.
    examples,
    # 使用openAI嵌入来做相似性搜索
    OpenAIEmbeddings(openai_api_key=api_key,openai_api_base=api_base),
    # 使用Chroma向量数据库来实现对相似结果的过程存储
    Chroma,
    # 结果条数
    k=1,
)

#使用小样本提示词模板
similar_prompt = FewShotPromptTemplate(
    # 传入选择器和模板以及前缀后缀和输入变量
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="给出每个输入词的反义词",
    suffix="原词: {adjective}\n反义:",
    input_variables=["adjective"],
)

# 输入一个形容感觉的词语，应该查找近似的 happy/sad 示例
print(similar_prompt.format(adjective="worried"))
```

### ChatOpenAI

```python
from langchain.chat_models import ChatOpenAI
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

chat = ChatOpenAI(
    model="gpt-4",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base
)

print(chat.predict("你好"))
```

```python
#调用chatmodels，以openai为例

from langchain.chat_models import ChatOpenAI
from langchain.schema.messages import HumanMessage,AIMessage
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

chat = ChatOpenAI(
    model="gpt-4",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base
)

messages = [
    AIMessage(role="system",content="你好，我是admin！"),
    HumanMessage(role="user",content="你好admin，我是小奇!"),
    AIMessage(role="system",content="认识你很高兴!"),
    HumanMessage(role="user",content="你能帮我做什么？")
]

response = chat.invoke(messages)
print(response)
```

### 流式调用

```python
#LLM类大模型的流式输出方法

from langchain.llms import OpenAI
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#构造一个llm
llm = OpenAI(
    model = "gpt-3.5-turbo-instruct",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
    max_tokens=512,
)

for chunk in llm.stream("写一首诗"):
    print(chunk,end="",flush=False)
```

```python
#chatmodels的流式调用方法
#使用clade模型
from langchain.chat_models import ChatOpenAI
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(
    model = "claude-3-opus-20240229",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
    max_tokens=512,
)
for chunk in llm.stream("写一首诗"):
    print(chunk,end="\n",flush=False)
```

### 追踪Token的使用

```python
#LLM的toekn追踪
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#构造一个llm
llm = OpenAI(
    model = "gpt-3.5-turbo-instruct",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
    max_tokens=512,
)

with get_openai_callback() as cb:
    result = llm.invoke("给我讲一个笑话")
    print(result)
    print(cb)
```

```python
#chatmodels的token追踪
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(
    model = "gpt-4",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
    max_tokens=512,
)

with get_openai_callback() as cb:
    result = llm.invoke("给我讲一个笑话")
    print(result)
    print(cb)
```

### 自定义输出

```python
# 以数据模型来输出
from langchain.llms import  OpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain.pydantic_v1 import BaseModel,Field,validator
from typing import  List
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#构造LLM
model = OpenAI(
    model = "gpt-3.5-turbo-instruct",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
)

#定义个数据模型，用来描述最终的实例结构
class Joke(BaseModel):
    setup:str = Field(description="设置笑话的问题")
    punchline:str = Field(description="回答笑话的答案")

    #验证问题是否符合要求
    @validator("setup")
    def question_mark(cls,field):
        if field[-1] != "？":
            raise ValueError("不符合预期的问题格式!")
        return field

#将Joke数据模型传入
parser = PydanticOutputParser(pydantic_object=Joke)


prompt = PromptTemplate(
    template = "回答用户的输入.\n{format_instructions}\n{query}\n",
    input_variables = ["query"],
    partial_variables = {"format_instructions":parser.get_format_instructions()}
)

prompt_and_model = prompt | model
out_put = prompt_and_model.invoke({"query":"给我讲一个笑话"})
print("out_put:",out_put)
parser.invoke(out_put)
```

```python
#LLM的输出格式化成python list形式，类似['a','b','c']

from langchain.output_parsers import  CommaSeparatedListOutputParser
from langchain.prompts import  PromptTemplate
from langchain.llms import OpenAI
import os
api_base = os.getenv("OPENAI_PROXY")
api_key = os.getenv("OPENAI_API_KEY")

#构造LLM
model = OpenAI(
    model = "gpt-3.5-turbo-instruct",
    temperature=0,
    openai_api_key = api_key,
    openai_api_base = api_base,
)

parser = CommaSeparatedListOutputParser()

prompt = PromptTemplate(
    template = "列出5个{subject}.\n{format_instructions}",
    input_variables = ["subject"],
    partial_variables = {"format_instructions":parser.get_format_instructions()}
)

_input = prompt.format(subject="推荐几个快速减肥的途径")
output = model(_input)
print(output)
#格式化
parser.parse(output)
```

## RAG

Retrieval-Augmented Generation    检索增强生成

LLM短板：知识内容不够新，没办法实时学习内容，更新比较慢，闭源数据无法感知。

通过RAG技术让LLM更聪明，帮助计算机更好的理解和回答问题。

<img src=".\picture\RAG.jpg" style="zoom:80%;" />

langchain中的RAG实现

![](D:\project\notes\AI\picture\langchain_rag.jpg)

### Loader机制

- 加载markdown

- 加载cvs

- 加载文件目录

- 加载html

- 加载JSON

- 加载PDF

### 文档转换器

 文档切割

原理

1. 将文档分成小的、有意义的块(句子).

2. 将小的块组合成为一个更大的块，直到达到一定的大小.

3. 一旦达到一定的大小，接着开始创建与下一个块重叠的部分.

文档的总结、精炼和翻译

### 如何处理长文本切分信息丢失问题?

论文资料：

[[2307.03172] Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)

对于长文本：10个以上块检索性能大幅下降，相关信息在头尾性能最高

解决方案：检索-排序(问题相关排在头尾)-使用

### 文本向量化

Embedding嵌入可以让一组文本或者一段话以向量来表示，从而可以让我们在向量空间中进行语义搜索之类的操作，从而大幅提升学习能力

### 向量数据库

向量数据库主要在信息索引和信息检索两个方向应用

向量数据:用空间来描述高维数据，以距离来判断亲疏

向量数据库处理高维数据方面具备天然优势

是图形处理、推荐系统的背后英雄

管理:以原始数据形式处理数据，管理更有效

存储:能够存储向量数据以及AI需要的高维数据

检索:可以高效检索数据，AI非常需要的特性
让AI具备了记忆能力

![](D:\project\notes\AI\picture\database.jpg)

### 几种检索优化方式

1.使用多重查询提高文档检索精确度

2.使用上下文压缩检索降低冗余信息

3.在向量存储里使用最大边际相似性（MMR）和相似性打分

## 大模型记忆增强

### 四种内置链

#### LLMChain

最常用的链式
提示词模板+(LLM/chatModes)+输出格式化器(可选)

支持多种调用方式

#### SequentialChain

顺序执行

将前一个LLM的输出作为下一个LLM的输入

#### RouterChain

路由链支持创建一个非确定性链，由LLM来选择下一步

链内的多个prompts模板描述了不同的提示请求

#### Transformation

支持对传递部件的转换

比如将一个超长文本过滤转换为仅包含前三个段落，然后提交给LLM

#### 官方提供的链

[langchain-hub/chains at master · hwchase17/langchain-hub · GitHub](https://github.com/hwchase17/langchain-hub/tree/master/chains)

### 四种处理文档预制链

#### stuffChain

最常见的文档链，将文档直接塞进prompt中，为LLM回答问题提供上下文资料，适合小文档场景

#### refine

通过循环引用LLM，将文档不断投喂，并产生各种中间答案，适合逻辑有上下文关联的文档，不适合交叉引用的文档

#### Map reduce

先将每个文档或文档块分别投喂给LLM，并得到结果集（Map步骤），然后通过一个文档合并链，获得一个输出结果（Reduce步骤）

#### Map re-rank

先将每个文档或文档块投喂给LLM,并对每个文档或文档块生成问题的答案进行打分，然后将打分最高的文档或文档块作为最终答案返回

### 不同的Memory工具

利用内存实现短时记忆

利用Entity memory构建实体记忆

利用知识图谱来构建记忆

利用对话摘要来兼容内存中的长对话

使用token来刷新内存缓冲区

使用向量数据库实现长时记忆

### 在链上使用记忆

- LLMChain

- ConversationChain

- 自定义

- 同一个链合并使用多个记忆

- 给一个多参数链增加记忆

## Agents

### 几种不同的agent类型

- ZERO_SHOT_REACT_DESCRIPTION

- CHAT_ZERO_SHOT_REACT_DESCRIPTION

- CONVERSATIONAL_REACT_DESCRIPTION

- CHAT_CONVERSATIONAL_REACT_DESCRIPTION

- OPENAI_FUNCTIONS

- STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION

ZERO_SHOT_REACT_DESCRIPTION

零样本增强式生成，即在没有示例的情况下可以自主的进行对话的类型。https://blog.csdn.net/zcyzcyjava/article/details/127006287 [零样本、单样本、少样本]

CHAT_ZERO_SHOT_REACT_DESCRIPTION 使用了chatmodel

CONVERSATIONAL_REACT_DESCRIPTION 一个对话型的agent，这个agent要求与memory一起使用

CHAT_CONVERSATIONAL_REACT_DESCRIPTION 使用了chatmodel

OPENAI_FUNCTIONS，使用openai的函数调用来实现的，只支持openai模型。

STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION 对输出做了结构化处理

### 如何给agent正确的增加记忆

- 将memory插入到提示词模板中

### 在agent与tool之间共享记忆

- 自定义一个工具用来LLMChain来总结内容

- 使用readonlymemory来共享记忆

- 观察共享与不共享的区别

### 如何加载使用tool

- 加载预制tool的方法

- 几种tool的使用方式

langchain预制了大量的tools，基本这些工具能满足大部分需求，https://github.com/langchain-ai/langchain/tree/v0.0.352/docs/docs/integrations/tools

```
SerpAPI
最常见的聚合搜索引擎 https://serper.dev/dashboard

使用Dall-E
Dall-E是openai出品的文到图AI大模型

Eleven Labs Text2Speech
ElevenLabs 是非常优秀的TTS合成API

GraphQL
一种api查询语言，类似sql，我们用它来查询奈飞的数据库，查找一下和星球大战相关的电影，
API地址https://swapi-graphql.netlify.app/.netlify/functions/index
```

### Tookit

tookit是langchain已经封装好的一系列工具，一个工具包是一组工具来组合完成特定的任务

Azure认知服务 https://portal.azure.com/#allservices

- AzureCogsFormRecognizerTool：从文档里提取文本

- AzureCogsSpeech2TextTool：语音到文本

- AzureCogsText2SpeechTool：文本到语音

### LCEL(LangChain Expression Language)

一种在langchain之上封装的高级解释语言

简化链条开发，支持真实生产环境而发明

## 常用网站

模型仓库

https://huggingface.co/
