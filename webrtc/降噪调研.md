# 降噪调研

## 需求

在音乐课上，录制声音，通过降噪技术，降低周围的噪音。

在对需要的语音造成最小失真的前提下，尽可能的将噪声从含噪信号中去除。



## 技术选型

### Speex

Speex 是一套主要针对语音的开源免费，无专利保护的应用集合，Speex项目旨在通过免费提供昂贵专有语音编解码器的替代方案来降低语音应用程序的进入壁垒。它的主要功能有同一位流中的窄带（8 kHz），宽带（16 kHz）和超宽带（32 kHz）压缩 ,强度立体声编码,丢包隐藏,可变比特率操作（VBR）,语音活动检测（VAD）,不连续传输（DTX）,定点端口,回声消除器,噪声抑制。开发Speex的Xiph.org基金会已经宣布废弃Speex，建议改用Opus取代。

### WebRtc

它支持视频，语音，和同龄人之间发送通用数据，允许开发者建立强大的语音和视频通信解决方案。该技术适用于所有现代浏览器，以及对所有主要平台的本地客户。背后的WebRTC的技术实现为一个开放的网络标准，可作为所有主流浏览器常规的JavaScript API。对于本地客户端，如Android和iOS应用中，库可提供相同的功能。webRtc库里有一个音频降噪的模块。

### rnnoise

根据噪声的不同大部分处理是针对平稳噪声以及瞬时噪声来做。RNNoise 降噪算法则是根据纯语音以及噪声通过 GRU 训练来做。包含特征点提取、预料等核心部分。





传统降噪算法大部分是估计噪声 + 维纳滤波，噪声估计的准确性是整个算法效果的核心。

RNNoise 的优点主要是一个算法通过训练可以解决所有噪声场景以及可以优化传统噪声估计的时延和收敛问题。
RNNoise 的缺点是深度学习算法落地问题。因为相对大部分传统算法，RNNoise 训练要得到一个很好的效果，由于特征点个数、隐藏单元的个数以及神经网络层数的增加，导致模型增大，运行效率。

所以对比过来，**webrtc的降噪模块比较适合业务使用**


## webrtc简介

[webrtc 官方架构图介绍](https://webrtc.github.io/webrtc-org/architecture/)

<img src=".\assets\webrtc_architecture.png" style="zoom: 80%;" />



从整体的框架来看，webrtc代码容量有非常大，但是各个引擎之间是相互独立的。所以我们可以单独把音频模块相应的功能抽取出来，主要抽出了**ns（降噪）**和 **agc (增益)**模块。





## 降噪原理解析

webRTC是对**似然比函数**进行改进，将多个语音/噪声分类特征合并到一个模型中形成一个多特征综合概率密度函数，对输入的每帧频谱进行分析。其可以有效抑制风扇/办公设备等噪声。



其抑制过程如下：

对接收到的每一帧带噪语音信号，以对该帧的初始噪声估计为前提，定义语音概率函数，测量每一帧带噪信号的分类特征，使用测量出来的分类特征，计算每一帧基于多特征的语音概率，在对计算出的语音概率进行动态因子（信号分类特征和阈值参数）加权，根据计算出的每帧基于特征的语音概率，修改多帧中每一帧的语音概率函数，以及使用修改后每帧语音概率函数，更新每帧中的初始噪声（连续多帧中每一帧的分位数噪声）估计。




## 注意事项

在源码里提到，音频处理时WebRTC一次仅能处理10ms数据。

```cpp
/*
 * This functions does noise suppression for the inserted speech frame. The
 * input and output signals should always be 10ms (80 or 160 samples).
 *
 * Input
 *      - nsxInst       : NSx instance. Needs to be initiated before call.
 *      - speechFrame   : Pointer to speech frame buffer for each band
 *      - num_bands     : Number of bands
 *
 * Output:
 *      - nsxInst       : Updated NSx instance
 *      - outFrame      : Pointer to output frame for each band
 */
void WebRtcNsx_Process(NsxHandle *nsxInst,
                       const int16_t *const *speechFrame,
                       int num_bands,
                       int16_t *const *outFrame);
```

采样点数=采样率×时间（秒）

对于8000采样率，16bit的音频数据，10ms的时间采样点就是80个，一个采样点16bit也就是两个字节，那么需要传入WebRtcNsx_Process的数据就是160字节。

对于16000采样率，16bit的音频数据，10ms的时间采样点就是160个，一个采样点16bit也就是两个字节，那么需要传入WebRtcNsx_Process的数据就是320字节。





## 参考文献

[希沃白板移动开发](https://juejin.cn/user/2137106337501533/posts)

[单独抽取webRtc的NS&NSX(降噪)模块](https://www.jianshu.com/p/72ae0ca2c0a7)

[单独抽取webRtc的AGC(增益)模块](https://www.jianshu.com/p/e105a373d6d9)

[Android集成webrtc降噪和增益模块](https://blog.csdn.net/qq_38366777/article/details/107877262)



[webrtc](https://webrtc.googlesource.com/src/+/refs/heads/master/modules/audio_processing/ns/)

[AudioAndVideo](https://github.com/guoxiucai/AudioAndVideo)

[Android-NoiseSupression-](https://github.com/adzcsx2/Android-NoiseSupression-)

[WebRtcNsAgcModel](https://github.com/inodevip/WebRtcNsAgcModel)

